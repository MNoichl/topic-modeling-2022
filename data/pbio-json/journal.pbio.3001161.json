{
  "schema": "https://data.sciveyor.com/schema",
  "version": 5,
  "id": "doi:10.1371/journal.pbio.3001161",
  "doi": "10.1371/journal.pbio.3001161",
  "externalIds": [
    "pii:PBIOLOGY-D-20-03090",
    "pmid:33788834",
    "pmcid:PMC8041175"
  ],
  "license": "This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",
  "licenseUrl": "http://creativecommons.org/licenses/by/4.0/",
  "dataSource": "Public Library of Science",
  "dataSourceUrl": "https://data.sciveyor.com/source/plos",
  "dataSourceVersion": 1,
  "type": "article",
  "title": "Creating clear and informative image-based figures for scientific publications",
  "authors": [
    {
      "name": "Helena Jambor",
      "first": "Helena",
      "last": "Jambor",
      "affiliation": "Mildred Scheel Early Career Center, Medical Faculty, Technische Universität Dresden, Dresden, Germany",
      "externalIds": [
        "orcid:https://orcid.org/0000-0003-3397-1842"
      ]
    },
    {
      "name": "Alberto Antonietti",
      "first": "Alberto",
      "last": "Antonietti",
      "affiliation": "Department of Electronics, Information and Bioengineering, Politecnico di Milano, Italy; Department of Brain and Behavioral Sciences, University of Pavia, Pavia, Italy",
      "externalIds": [
        "orcid:https://orcid.org/0000-0003-0388-6321"
      ]
    },
    {
      "name": "Bradly Alicea",
      "first": "Bradly",
      "last": "Alicea",
      "affiliation": "Orthogonal Research and Education Laboratory, Champaign, IL, United States of America",
      "externalIds": [
        "orcid:https://orcid.org/0000-0003-3869-3175"
      ]
    },
    {
      "name": "Tracy L. Audisio",
      "first": "Tracy L.",
      "last": "Audisio",
      "affiliation": "Evolutionary Genomics Unit, Okinawa Institute of Science and Technology, Okinawa, Japan",
      "externalIds": [
        "orcid:https://orcid.org/0000-0002-4663-8101"
      ]
    },
    {
      "name": "Susann Auer",
      "first": "Susann",
      "last": "Auer",
      "affiliation": "Department of Plant Physiology, Faculty of Biology, Technische Universität Dresden, Dresden, Germany",
      "externalIds": [
        "orcid:https://orcid.org/0000-0001-6566-5060"
      ]
    },
    {
      "name": "Vivek Bhardwaj",
      "first": "Vivek",
      "last": "Bhardwaj",
      "affiliation": "Max Plank Institute of Immunology and Epigenetics, Freiburg, Germany; Hubrecht Institute, Utrecht, the Netherlands",
      "externalIds": [
        "orcid:https://orcid.org/0000-0002-5570-9338"
      ]
    },
    {
      "name": "Steven J. Burgess",
      "first": "Steven J.",
      "last": "Burgess",
      "affiliation": "Carl R Woese Institute for Genomic Biology, University of Illinois at Urbana-Champaign, Urbana, IL, United States of America",
      "externalIds": [
        "orcid:https://orcid.org/0000-0003-2353-7794"
      ]
    },
    {
      "name": "Iuliia Ferling",
      "first": "Iuliia",
      "last": "Ferling",
      "affiliation": "Junior Research Group Evolution of Microbial Interactions, Leibniz Institute for Natural Product Research and Infection Biology—Hans Knöll Institute (HKI), Jena, Germany",
      "externalIds": [
        "orcid:https://orcid.org/0000-0002-5028-5847"
      ]
    },
    {
      "name": "Małgorzata Anna Gazda",
      "first": "Małgorzata Anna",
      "last": "Gazda",
      "affiliation": "CIBIO/InBIO, Centro de Investigação em Biodiversidade e Recursos Genéticos, Campus Agrário de Vairão, Universidade do Porto, Vairão, Portugal; Departamento de Biologia, Faculdade de Ciências, Universidade do Porto, Porto, Portugal",
      "externalIds": [
        "orcid:https://orcid.org/0000-0001-8369-1350"
      ]
    },
    {
      "name": "Luke H. Hoeppner",
      "first": "Luke H.",
      "last": "Hoeppner",
      "affiliation": "The Hormel Institute, University of Minnesota, Austin, MN, United States of America; The Masonic Cancer Center, University of Minnesota, Minneapolis, MN, United States of America",
      "externalIds": [
        "orcid:https://orcid.org/0000-0003-3948-4244"
      ]
    },
    {
      "name": "Vinodh Ilangovan",
      "first": "Vinodh",
      "last": "Ilangovan",
      "affiliation": "Aarhus University, Aarhus, Denmark"
    },
    {
      "name": "Hung Lo",
      "first": "Hung",
      "last": "Lo",
      "affiliation": "Neuroscience Research Center, Charité—Universitätsmedizin Berlin, Corporate member of Freie Universität Berlin, Humboldt—Universität zu Berlin, Berlin Institute of Health, Berlin, Germany; Einstein Center for Neurosciences Berlin, Berlin, Germany",
      "externalIds": [
        "orcid:https://orcid.org/0000-0003-4808-6440"
      ]
    },
    {
      "name": "Mischa Olson",
      "first": "Mischa",
      "last": "Olson",
      "affiliation": "Section of Plant Biology, School of Integrative Plant Science, Cornell University, Ithaca, NY, United States of America",
      "externalIds": [
        "orcid:https://orcid.org/0000-0002-7606-3572"
      ]
    },
    {
      "name": "Salem Yousef Mohamed",
      "first": "Salem Yousef",
      "last": "Mohamed",
      "affiliation": "Gastroenterology and Hepatology Unit, Internal Medicine Department, Faculty of Medicine, University of Zagazig, Zagazig, Egypt"
    },
    {
      "name": "Sarvenaz Sarabipour",
      "first": "Sarvenaz",
      "last": "Sarabipour",
      "affiliation": "Institute for Computational Medicine and the Department of Biomedical Engineering, Johns Hopkins University, Baltimore, MD, United States of America",
      "externalIds": [
        "orcid:https://orcid.org/0000-0001-5097-5509"
      ]
    },
    {
      "name": "Aalok Varma",
      "first": "Aalok",
      "last": "Varma",
      "affiliation": "National Centre for Biological Sciences (NCBS), Tata Institute of Fundamental Research (TIFR), Bangalore, Karnataka, India",
      "externalIds": [
        "orcid:https://orcid.org/0000-0002-7869-6015"
      ]
    },
    {
      "name": "Kaivalya Walavalkar",
      "first": "Kaivalya",
      "last": "Walavalkar",
      "affiliation": "National Centre for Biological Sciences (NCBS), Tata Institute of Fundamental Research (TIFR), Bangalore, Karnataka, India"
    },
    {
      "name": "Erin M. Wissink",
      "first": "Erin M.",
      "last": "Wissink",
      "affiliation": "Department of Molecular Biology and Genetics, Cornell University, Ithaca, NY, United States of America",
      "externalIds": [
        "orcid:https://orcid.org/0000-0003-1054-4899"
      ]
    },
    {
      "name": "Tracey L. Weissgerber",
      "first": "Tracey L.",
      "last": "Weissgerber",
      "affiliation": "Berlin Institute of Health at Charité–Universitätsmedizin Berlin, QUEST Center, Berlin, Germany",
      "externalIds": [
        "orcid:https://orcid.org/0000-0002-7490-2600"
      ]
    }
  ],
  "journal": "PLOS Biology",
  "date": "2021-03-31",
  "dateAccepted": "2021-02-26",
  "dateReceived": "2020-10-19",
  "volume": "19",
  "number": "3",
  "pages": "e3001161",
  "tags": [
    "Careers in research",
    "Cell biology",
    "Cell physiology",
    "Diagnostic medicine",
    "Diagnostic radiology",
    "Discipline-v3/Biology and life sciences",
    "Discipline-v3/Careers in research",
    "Discipline-v3/Cell biology",
    "Discipline-v3/Cell physiology",
    "Discipline-v3/Diagnostic medicine",
    "Discipline-v3/Diagnostic radiology",
    "Discipline-v3/Fluorescence imaging",
    "Discipline-v3/Imaging techniques",
    "Discipline-v3/Magnetic resonance imaging",
    "Discipline-v3/Medicine and health sciences",
    "Discipline-v3/People and places",
    "Discipline-v3/Physiology",
    "Discipline-v3/Plant physiology",
    "Discipline-v3/Plant science",
    "Discipline-v3/Population groupings",
    "Discipline-v3/Professions",
    "Discipline-v3/Radiology and imaging",
    "Discipline-v3/Research and analysis methods",
    "Discipline-v3/Science and technology workforce",
    "Discipline-v3/Science policy",
    "Discipline-v3/Scientists",
    "Discipline-v3/Ultrasound imaging",
    "Fluorescence imaging",
    "Imaging techniques",
    "Magnetic resonance imaging",
    "Physiology",
    "Plant physiology",
    "Plant science",
    "Population groupings",
    "Professions",
    "Radiology and imaging",
    "Science and technology workforce",
    "Scientists",
    "Type/Meta-Research Article",
    "Ultrasound imaging"
  ],
  "abstract": "Scientists routinely use images to display data. Readers often examine figures first; therefore, it is important that figures are accessible to a broad audience. Many resources discuss fraudulent image manipulation and technical specifications for image acquisition; however, data on the legibility and interpretability of images are scarce. We systematically examined these factors in non-blot images published in the top 15 journals in 3 fields; plant sciences, cell biology, and physiology (n = 580 papers). Common problems included missing scale bars, misplaced or poorly marked insets, images or labels that were not accessible to colorblind readers, and insufficient explanations of colors, labels, annotations, or the species and tissue or object depicted in the image. Papers that met all good practice criteria examined for all image-based figures were uncommon (physiology 16%, cell biology 12%, plant sciences 2%). We present detailed descriptions and visual examples to help scientists avoid common pitfalls when publishing images. Our recommendations address image magnification, scale information, insets, annotation, and color and may encourage discussion about quality standards for bioimage publishing.",
  "fullText": "Introduction\n\nImages are often used to share scientific data, providing the visual evidence needed to turn concepts and hypotheses into observable findings. An analysis of 8 million images from more than 650,000 papers deposited in PubMed Central revealed that 22.7% of figures were “photographs,” a category that included microscope images, diagnostic images, radiology images, and fluorescence images [1]. Cell biology was one of the most visually intensive fields, with publications containing an average of approximately 0.8 photographs per page [1]. Plant sciences papers included approximately 0.5 photographs per page [1].\n\nWhile there are many resources on fraudulent image manipulation and technical requirements for image acquisition and publishing [2–4], data examining the quality of reporting and ease of interpretation for image-based figures are scarce. Recent evidence suggests that important methodological details about image acquisition are often missing [5]. Researchers generally receive little or no training in designing figures; yet many scientists and editors report that figures and tables are one of the first elements that they examine when reading a paper [6,7]. When scientists and journals share papers on social media, posts often include figures to attract interest. The PubMed search engine caters to scientists’ desire to see the data by presenting thumbnail images of all figures in the paper just below the abstract [8]. Readers can click on each image to examine the figure, without ever accessing the paper or seeing the introduction or methods. EMBO’s Source Data tool (RRID:SCR_015018) allows scientists and publishers to share or explore figures, as well as the underlying data, in a findable and machine readable fashion [9].\n\nImage-based figures in publications are generally intended for a wide audience. This may include scientists in the same or related fields, editors, patients, educators, and grants officers. General recommendations emphasize that authors should design figures for their audience rather than themselves and that figures should be self-explanatory [7]. Despite this, figures in papers outside one’s immediate area of expertise are often difficult to interpret, marking a missed opportunity to make the research accessible to a wide audience. Stringent quality standards would also make image data more reproducible. A recent study of fMRI image data, for example, revealed that incomplete documentation and presentation of brain images led to nonreproducible results [10,11].\n\nHere, we examined the quality of reporting and accessibility of image-based figures among papers published in top journals in plant sciences, cell biology, and physiology. Factors assessed include the use of scale bars, explanations of symbols and labels, clear and accurate inset markings, and transparent reporting of the object or species and tissue shown in the figure. We also examined whether images and labels were accessible to readers with the most common form of color blindness [12]. Based on our results, we provide targeted recommendations about how scientists can create informative image-based figures that are accessible to a broad audience. These recommendations may also be used to establish quality standards for images deposited in emerging image data repositories.\n\nResults\n\nUsing a science of science approach to investigate current practices\n\nThis study was conducted as part of a participant-guided learn-by-doing course, in which eLife Community Ambassadors from around the world worked together to design, complete, and publish a meta-research study [13]. Participants in the 2018 Ambassadors program designed the study, developed screening and abstraction protocols, and screened papers to identify eligible articles (HJ, BA, SJB, VB, LHH, VI, SS, EMW). Participants in the 2019 Ambassadors program refined the data abstraction protocol, completed data abstraction and analysis, and prepared the figures and manuscript (AA, SA, TLA, IF, MAG, HL, SYM, MO, AV, KW, HJ, TLW).\n\nTo investigate current practices in image publishing, we selected 3 diverse fields of biology to increase generalizability. For each field, we examined papers published in April 2018 in the top 15 journals, which publish original research (S1–S3 Tables). All full-length original research articles that contained at least one photograph, microscope image, electron microscope image, or clinical image (MRI, ultrasound, X-ray, etc.) were included in the analysis (S1 Fig). Blots and computer-generated images were excluded, as some of the criteria assessed do not apply to these types of images. Two independent reviewers assessed each paper, according to the detailed data abstraction protocol (see methods and information deposited on the Open Science Framework (OSF) (RRID:SCR_017419) at https://doi.org/10.17605/OSF.IO/B5296) [14]. The repository also includes data, code, and figures.\n\nImage analysis\n\nFirst, we confirmed that images are common in the 3 biology subfields analyzed. More than half of the original research articles in the sample contained images (plant science: 68%, cell biology: 72%, physiology: 55%). Among the 580 papers that included images, microscope images were very common in all 3 fields (61% to 88%, Fig 1A). Photographs were very common in plant sciences (86%), but less widespread in cell biology (38%) and physiology (17%). Electron microscope images were less common in all 3 fields (11% to 19%). Clinical images, such as X-rays, MRI or ultrasound, and other types of images were rare (2% to 9%).\n\nScale information is essential to interpret biological images. Approximately half of papers in physiology (49%) and cell biology (55%) and 28% of plant science papers provided scale bars with dimensions (in the figure or legend) for all images in the paper (Fig 1B, S4 Table). Approximately one-third of papers in each field contained incomplete scale information, such as reporting magnification or presenting scale information for a subset of images. Twenty-four percent of physiology papers, 10% of cell biology papers, and 29% of plant sciences papers contained no scale information on any image.\n\nSome publications use insets to show the same image at 2 different scales (cell biology papers: 40%, physiology: 17%, plant sciences: 12%). In this case, the authors should indicate the position of the high-magnification inset in the low-magnification image. The majority of papers in all 3 fields clearly and accurately marked the location of all insets (53% to 70%; Fig 1C, left panel); however, one-fifth of papers appeared to have marked the location of at least one inset incorrectly (17% to 22%). Clearly visible inset markings were missing for some or all insets in 13% to 28% of papers (Fig 1C, left panel). Approximately half of papers (43% to 53%; Fig 1C, right panel) provided legend explanations or markings on the figure to clearly show that an inset was used, whereas this information was missing for some or all insets in the remaining papers.\n\nMany images contain information in color. We sought to determine whether color images were accessible to readers with deuteranopia, the most common form of color blindness, by using the color blindness simulator Color Oracle (https://colororacle.org/, RRID: SCR_018400). We evaluated only images in which the authors selected the image colors (e.g., fluorescence microscopy). Papers without any colorblind accessible figures were uncommon (3% to 6%); however, 45% of cell biology papers and 21% to 24% of physiology and plant science papers contained some images that were inaccessible to readers with deuteranopia (Fig 2A). Seventeen percent to 34% of papers contained color annotations that were not visible to someone with deuteranopia.\n\nFigure legends and, less often, titles typically provide essential information needed to interpret an image. This text provides information on the specimen and details of the image, while also explaining labels and annotations used to highlight structures or colors. Fifty-seven percent of physiology papers, 48% of cell biology papers, and 20% of plant papers described the species and tissue or object shown completely. Five percent to 17% of papers did not provide any such information (Fig 2B). Approximately half of the papers (47% to 58%; Fig 1C, right panel) also failed or partially failed to adequately explain that insets were used. Annotations of structures were better explained. Two-thirds of papers across all 3 fields clearly stated the meaning of all image labels, while 18% to 24% of papers provided partial explanations. Most papers (73% to 83%) completely explained the image colors by stating what substance each color represented or naming the dyes or staining technique used.\n\nFinally, we examined the number of papers that used optimal image presentation practices for all criteria assessed in the study. Twenty-eight (16%) physiology papers, 19 (12%) cell biology papers, and 6 (2%) plant sciences papers met all criteria for all image-based figures in the paper. In plant sciences and physiology, the most common problems were with scale bars, insets, and specifying in the legend the species and tissue or object shown. In cell biology, the most common problems were with insets, colorblind accessibility, and specifying in the legend the species and tissue or object shown.\n\nDesigning image-based figures: How can we improve?\n\nOur results obtained by examining 580 papers from 3 fields provide us with unique insights into the quality of reporting and the accessibility of image-based figures. Our quantitative description of standard practices in image publication highlights opportunities to improve transparency and accessibility to readers from different backgrounds. We have therefore outlined specific actions that scientists can take when creating images, designing multipanel figures, annotating figures, and preparing figure legends.\n\nThroughout the paper, we provide visual examples to illustrate each stage of the figure preparation process. Other elements are often omitted to focus readers’ attention on the step illustrated in the figure. For example, a figure that highlights best practices for displaying scale bars may not include annotations designed to explain key features of the image. When preparing image-based figures in scientific publications, readers should address all relevant steps in each figure. All steps described below (image cropping and insets, adding scale bars and annotation, choosing color channel appearances, figure panel layout) can be implemented with standard image processing software such as FIJI [15] (RRID:SCR_002285) and ImageJ2 [16] (RRID:SCR_003070), which are open source, free programs for bioimage analysis. A quick guide on how to do basic image processing for publications with FIJI is available in a recent cheat sheet publication [17], and a discussion forum and wiki are available for FIJI and ImageJ (https://imagej.net/).\n\n1. Choose a scale or magnification that fits your research question\n\nScientists should select an image scale or magnification that allows readers to clearly see features needed to answer the research question. Fig 3A [18] shows Drosophila melanogaster at 3 different microscopic scales. The first focuses on the ovary tissue and might be used to illustrate the appearance of the tissue or show stages of development. The second focuses on a group of cells. In this example, the “egg chamber” cells show different nucleic acid distributions. The third example focuses on subcellular details in one cell, for example, to show finer detail of RNA granules or organelle shape.\n\nWhen both low and high magnifications are necessary for one image, insets are used to show a small portion of the image at higher magnification (Fig 3B, [19]). The inset location must be accurately marked in the low-magnification image. We observed that the inset position in the low-magnification image was missing, unclear, or incorrectly placed in approximately one-third of papers. Inset positions should be clearly marked by lines or regions of interest in a high-contrast color, usually black or white. Insets may also be explained in the figure legend. Care must be taken when preparing figures outside vector graphics suits, as insert positions may move during file saving or export.\n\n2. Include a clearly labeled scale bar\n\nScale information allows audiences to quickly understand the size of features shown in images. This is especially important for microscopic images where we have no intuitive understanding of scale. Scale information for photographs should be considered when capturing images as rulers are often placed into the frame. Our analysis revealed that 10% to 29% of papers screened failed to provide any scale information and that another third only provided incomplete scale information (Fig 1B). Scientists should consider the following points when displaying scale bars:\n\nEvery image type needs a scale bar: Authors usually add scale bars to microscope images but often leave them out in photos and clinical images, possibly because these depict familiar objects such a human or plant. Missing scale bars, however, adversely affect reproducibility. A size difference of 20% in between a published study and the reader’s lab animals, for example, could impact study results by leading to an important difference in phenotype. Providing scale bars allows scientists to detect such discrepancies and may affect their interpretation of published work. Scale bars may not be a standard feature of image acquisition and processing software for clinical images. Authors may need to contact device manufacturers to determine the image size and add height and width labels.\n\nScale bars and labels should be clearly visible: Short scale bars, thin scale bars, and scale bars in colors that are similar to the image color can easily be overlooked (Fig 4). In multicolor images, it can be difficult to find a color that makes the scale bar stand out. Authors can solve this problem by placing the scale bar outside the image or onto a box with a more suitable background color.\n\nAnnotate scale bar dimensions on the image: Stating the dimensions along with the scale bar allows readers to interpret the image more quickly. Despite this, dimensions were typically stated in the legend instead (Fig 1B), possibly a legacy of printing processes that discouraged text in images. Dimensions should be in high resolution and large enough to be legible. In our set, we came across small and/or low-resolution annotations that were illegible in electronic versions of the paper, even after zooming in. Scale bars that are visible on larger figures produced by authors may be difficult to read when the size of the figure is reduced to fit onto a journal page. Authors should carefully check page proofs to ensure that scale bars and dimensions are clearly visible.\n\n3. Use color wisely in images\n\nColors in images are used to display the natural appearance of an object or to visualize features with dyes and stains. In the scientific context, adapting colors is possible and may enhance readers’ understanding, while poor color schemes may distract or mislead. Images showing the natural appearance of a subject, specimen, or staining technique (e.g., images showing plant size and appearance, or histopathology images of fat tissue from mice on different diets) are generally presented in color (Fig 5). Images showing electron microscope images are captured in black and white (“grayscale”) by default and may be kept in grayscale to leverage the good contrast resulting from a full luminescence spectrum.\n\nIn some instances, scientists can choose whether to show grayscale or color images. Assigning colors may be optional, even though it is the default setting in imaging programs. When showing only one color channel, scientists may consider presenting this channel in grayscale to optimally display fine details. This may include variations in staining intensity or fine structures. When opting for color, authors should use grayscale visibility tests (Fig 6) to determine whether visibility is compromised. This can occur when dark colors, such as magenta, red, or blue, are shown on a black background.\n\n4. Choose a colorblind accessible color palette\n\nFluorescent images with merged color channels visualize the colocalization of different markers. While many readers find these images to be visually appealing and informative, these images are often inaccessible to colorblind coauthors, reviewers, editors, and readers. Deuteranopia, the most common form of colorblindness, affects up to 8% of men and 0.5% of women of northern European ancestry [12]. A study of articles published in top peripheral vascular disease journals revealed that 85% of papers with color maps and 58% of papers with heat maps used color palettes that were not colorblind safe [20]. We show that approximately half of cell biology papers, and one-third of physiology papers and plant science papers, contained images that were inaccessible to readers with deuteranopia. Scientists should consider the following points to ensure that images are accessible to colorblind readers.\n\nSelect colorblind safe colors: Researchers should use colorblind safe color palettes for fluorescence and other images where color may be adjusted. Fig 7 illustrates how 4 different color combinations would look to viewers with different types of color blindness. Green and red are indistinguishable to readers with deuteranopia, whereas green and blue are indistinguishable to readers with tritanopia, a rare form of color blindness. Cyan and magenta are the best options, as these 2 colors look different to viewers with normal color vision, deuteranopia, or tritanopia. Green and magenta are also shown, as scientists often prefer to show colors close to the excitation value of the fluorescent dyes, which are often green and red.\n\nDisplay separate channels in addition to the merged image: Selecting a colorblind safe color palette becomes increasingly difficult as more colors are added. When the image includes 3 or more colors, authors are encouraged to show separate images for each channel, followed by the merged image (Fig 8). Individual channels may be shown in grayscale to make it easier for readers to perceive variations in staining intensity.\n\nUse simulation tools to confirm that essential features are visible to colorblind viewers: Free tools, such as Color Oracle (RRID:SCR_018400), quickly simulate different forms of color blindness by adjusting the colors on the computer screen to simulate what a colorblind person would see. Scientists using FIJI (RRID:SCR002285) can select the “Simulate colorblindness” option in the “Color” menu under “Images.”\n\n5. Design the figure\n\nFigures often contain more than one panel. Careful planning is needed to convey a clear message, while ensuring that all panels fit together and follow a logical order. A planning table (Fig 9A) helps scientists to determine what information is needed to answer the research question. The table outlines the objectives, types of visualizations required, and experimental groups that should appear in each panel. A planning table template is available on OSF [14]. After completing the planning table, scientists should sketch out the position of panels and the position of images, graphs, and titles within each panel (Fig 9B). Audiences read a page either from top to bottom and/or from left to right. Selecting one reading direction and arranging panels in rows or columns helps with figure planning. Using enough white space to separate rows or columns will visually guide the reader through the figure. The authors can then assemble the figure based on the draft sketch.\n\n6. Annotate the figure\n\nAnnotations with text, symbols, or lines allow readers from many different backgrounds to rapidly see essential features, interpret images, and gain insight. Unfortunately, scientists often design figures for themselves, rather than their audience [7]. Examples of annotations are shown in Fig 10. Table 1 describes important factors to consider for each annotation type.\n\nWhen adding annotations to an image, scientists should consider the following steps.\n\nChoose the right amount of labeling. Fig 11 shows 3 levels of annotation. The barely annotated image (Fig 11A) is only accessible to scientists already familiar with the object and technique, whereas the heavily annotated version (Fig 11C) contains numerous annotations that obstruct the image and a legend that is time consuming to interpret. Fig 11B is more readable; annotations of a few key features are shown, and the explanations appear right below the image for easy interpretation. Explanations of labels are often placed in the figure legend. Alternating between examining the figure and legend is time consuming, especially when the legend and figure are on different pages. Fig 11D shows one option for situations where extensive annotations are required to explain a complex image. An annotated image is placed as a legend next to the original image. A semitransparent white layer mutes the image to allow annotations to stand out.\n\nUse abbreviations cautiously: Abbreviations are commonly used for image and figure annotation to save space but inevitably require more effort from the reader. Abbreviations are often ambiguous, especially across fields. Authors should run a web search for the abbreviation [21]. If the intended meaning is not a top result, authors should refrain from using the abbreviation or clearly define the abbreviation on the figure itself, even if it is already defined elsewhere in the manuscript. Note that in Fig 11, abbreviations have been written out below the image to reduce the number of legend entries.\n\nExplain colors and stains: Explanations of colors and stains were missing in around 20% of papers. Fig 12 illustrates several problematic practices observed in our dataset, as well as solutions for clearly explaining what each color represents. This figure uses fluorescence images as an example; however, we also observed many histology images in which authors did not mention which stain was used. Authors should describe how stains affect the tissue shown or use annotations to show staining patterns of specific structures. This allows readers who are unfamiliar with the stain to interpret the image.\n\nEnsure that annotations are accessible to colorblind readers: Confirming that labels or annotations are visible to colorblind readers is important for both color and grayscale images (Fig 13). Up to one-third of papers in our dataset contained annotations or labels that would not have been visible to someone with deuteranopia. This occurred because the annotations blended in with the background (e.g., red arrows on green plants) or the authors use the same symbol in colors that are indistinguishable to someone with deuteranopia to mark different features. Fig 13 illustrates how to annotate a grayscale image so that it is accessible to color blind readers. Using text to describe colors is also problematic for colorblind readers. This problem can be alleviated by using colored symbols in the legend or by using distinctly shaped annotations such as open versus closed arrows, thin versus wide lines, or dashed versus solid lines. Color blindness simulators help in determining whether annotations are accessible to all readers.\n\n7. Prepare figure legends\n\nEach figure and legend are meant to be self-explanatory and should allow readers to quickly assess a paper or understand complex studies that combine different methodologies or model systems. To date, there are no guidelines for figure legends for images, as the scope and length of legends varies across journals and disciplines. Some journals require legends to include details on object, size, methodology, or sample size, while other journals require a minimalist approach and mandate that information should not be repeated in subsequent figure legends.\n\nOur data suggest that important information needed to interpret images was regularly missing from the figure or figure legend. This includes the species and tissue type, or object shown in the figure, clear explanations of all labels, annotations and colors, and markings or legend entries denoting insets. Presenting this information on the figure itself is more efficient for the reader; however, any details that are not marked in the figure should be explained in the legend.\n\nWhile not reporting species and tissue information in every figure legend may be less of an issue for papers that examine a single species and tissue, this is a major problem when a study includes many species and tissues, which may be presented in different panels of the same figure. Additionally, the scientific community is increasingly developing automated data mining tools, such as the Source Data tool, to collect and synthesize information from figures and other parts of scientific papers. Unlike humans, these tools cannot piece together information scattered throughout the paper to determine what might be shown in a particular figure panel. Even for human readers, this process wastes time. Therefore, we recommend that authors present information in a clear and accessible manner, even if some information may be repeated for studies with simple designs.\n\nDiscussion\n\nA flood of images is published every day in scientific journals and the number is continuously increasing. Of these, around 4% likely contain intentionally or accidentally duplicated images [3]. Our data show that, in addition, most papers show images that are not fully interpretable due to issues with scale markings, annotation, and/or color. This affects scientists’ ability to interpret, critique, and build upon the work of others. Images are also increasingly submitted to image archives to make image data widely accessible and permit future reanalyses. A substantial fraction of images that are neither human nor machine-readable lowers the potential impact of such archives. Based on our data examining common problems with published images, we provide a few simple recommendations, with examples illustrating good practices. We hope that these recommendations will help authors to make their published images legible and interpretable.\n\nLimitations: While most results were consistent across the 3 subfields of biology, findings may not be generalizable to other fields. Our sample included the top 15 journals that publish original research for each field. Almost all journals were indexed in PubMed. Results may not be generalizable to journals that are unindexed, have low impact factors, or are not published in English. Data abstraction was performed manually due to the complexity of the assessments. Error rates were 5% for plant sciences, 4% for physiology, and 3% for cell biology. Our assessments focused on factors that affect readability of image-based figures in scientific publications. Future studies may include assessments of raw images and meta-data to examine factors that affect reproducibility, such as contrast settings, background filtering, and processing history.\n\nActions journals can take to make image-based figures more transparent and easier to interpret\n\nThe role of journals in improving the quality of reporting and accessibility of image-based figures should not be overlooked. There are several actions that journals might consider.\n\nScreen manuscripts for figures that are not colorblind safe: Open source automated screening tools [22] may help journals to efficiently identify common color maps that are not colorblind safe.\n\nUpdate journal policies: We encourage journal editors to update policies regarding colorblind accessibility, scale bars, and other factors outlined in this manuscript. Importantly, policy changes should be accompanied by clear plans for implementation and enforcement. Meta-research suggests that changing journal policy, without enforcement or implementation plans, has limited effects on author behavior. Amending journal policies to require authors to report research resource identifiers (RRIDs), for example, increases the number of papers reporting RRIDs by 1% [23]. In a study of life sciences articles published in Nature journals, the percentage of animal studies reporting the Landis 4 criteria (blinding, randomization, sample size calculation, exclusions) increased from 0% to 16.4% after new guidelines were released [24]. In contrast, a randomized controlled trial of animal studies submitted to PLOS ONE demonstrated that randomizing authors to complete the ARRIVE checklist during submission did not improve reporting [25]. Some improvements in reporting of confidence intervals, sample size justification, and inclusion and exclusion criteria were noted after Psychological Science introduced new policies [26], although this may have been partially due to widespread changes in the field. A joint editorial series published in the Journal of Physiology and British Journal of Pharmacology did not improve the quality of data presentation or statistical reporting [27].\n\nReevaluate limits on the number of figures: Limitations on the number of figures originally stemmed from printing costs calculations, which are becoming increasingly irrelevant as scientific publishing moves online. Unintended consequences of these policies include the advent of large, multipanel figures. These figures are often especially difficult to interpret because the legend appears on a different page, or the figure combines images addressing different research questions.\n\nReduce or eliminate page charges for color figures: As journals move online, policies designed to offset the increased cost of color printing are no longer needed. The added costs may incentivize authors to use grayscale in cases where color would be beneficial.\n\nEncourage authors to explain labels or annotations in the figure, rather than in the legend: This is more efficient for readers.\n\nEncourage authors to share image data in public repositories: Open data benefits authors and the scientific community [28–30].\n\nHow can the scientific community improve image-based figures?\n\nThe role of scientists in the community is multifaceted. As authors, scientists should familiarize themselves with guidelines and recommendations, such as ours provided above. As reviewers, scientists should ask authors to improve erroneous or uninformative image-based figures. As instructors, scientists should ensure that bioimaging and image data handling is taught during undergraduate or graduate courses, and support existing initiatives such as NEUBIAS (Network of EUropean BioImage AnalystS) [31] that aim to increase training opportunities in bioimage analysis.\n\nScientists are also innovators. As such, they should support emerging image data archives, which may expand to automatically source images from published figures. Repositories for other types of data are already widespread; however, the idea of image repositories has only recently gained traction [32]. Existing image databases, which are mainly used for raw image data and meta-data, include the Allen Brain Atlas, the Image Data Resource [33], and the emerging BioImage Archives [32]. Springer Nature encourages authors to submit imaging data to the Image Data Resource [33]. While scientists have called for common quality standards for archived images and meta-data [32], such standards have not been defined, implemented, or taught. Examining standard practices for reporting images in scientific publications, as outlined here, is one strategy for establishing common quality standards.\n\nIn the future, it is possible that each image published electronically in a journal or submitted to an image data repository will follow good practice guidelines and will be accompanied by expanded “meta-data” or “alt-text/attribute” files. Alt-text is already published in html to provide context if an image cannot be accessed (e.g., by blind readers). Similarly, images in online articles and deposited in archives could contain essential information in a standardized format. The information could include the main objective of the figure, specimen information, ideally with RRID [34], specimen manipulation (dissection, staining, RRID for dyes and antibodies used), as well as the imaging method including essential items from meta-files of the microscope software, information about image processing and adjustments, information about scale, annotations, insets, and colors shown, and confirmation that the images are truly representative.\n\nConclusions\n\nOur meta-research study of standard practices for presenting images in 3 fields highlights current shortcomings in publications. Pubmed indexes approximately 800,000 new papers per year, or 2,200 papers per day (https://www.nlm.nih.gov/bsd/index_stats_comp.html). Twenty-three percent [1], or approximately 500 papers per day, contain images. Our survey data suggest that most of these papers will have deficiencies in image presentation, which may affect legibility and interpretability. These observations lead to targeted recommendations for improving the quality of published images. Our recommendations are available as a slide set via the OSF and can be used in teaching best practice to avoid misleading or uninformative image-based figures. Our analysis underscores the need for standardized image publishing guidelines. Adherence to such guidelines will allow the scientific community to unlock the full potential of image collections in the life sciences for current and future generations of researchers.\n\nMethods\n\nSystematic review\n\nWe examined original research articles that were published in April of 2018 in the top 15 journals that publish original research for each of 3 different categories (physiology, plant science, cell biology). Journals for each category were ranked according to 2016 impact factors listed for the specified categories in Journal Citation Reports. Journals that only publish review articles or that did not publish an April issue were excluded. We followed all relevant aspects of the PRISMA guidelines [35]. Items that only apply to meta-analyses or are not relevant to literature surveys were not followed. Ethical approval was not required.\n\nSearch strategy\n\nArticles were identified through a PubMed search, as all journals were PubMed indexed. Electronic search results were verified by comparison with the list of articles published in April issues on the journal website. The electronic search used the following terms:\n\nPhysiology: (&quot;Journal of pineal research&quot;[Journal] AND 3[Issue] AND 64[Volume]) OR (&quot;Acta physiologica (Oxford, England)&quot;[Journal] AND 222[Volume] AND 4[Issue]) OR (&quot;The Journal of physiology&quot;[Journal] AND 596[Volume] AND (7[Issue] OR 8[Issue])) OR ((&quot;American journal of physiology. Lung cellular and molecular physiology&quot;[Journal] OR &quot;American journal of physiology. Endocrinology and metabolism&quot;[Journal] OR &quot;American journal of physiology. Renal physiology&quot;[Journal] OR &quot;American journal of physiology. Cell physiology&quot;[Journal] OR &quot;American journal of physiology. Gastrointestinal and liver physiology&quot;[Journal]) AND 314[Volume] AND 4[Issue]) OR (“American journal of physiology. Heart and circulatory physiology”[Journal] AND 314[Volume] AND 4[Issue]) OR (&quot;The Journal of general physiology&quot;[Journal] AND 150[Volume] AND 4[Issue]) OR (&quot;Journal of cellular physiology&quot;[Journal] AND 233[Volume] AND 4[Issue]) OR (&quot;Journal of biological rhythms&quot;[Journal] AND 33[Volume] AND 2[Issue]) OR (&quot;Journal of applied physiology (Bethesda, Md.: 1985)&quot;[Journal] AND 124[Volume] AND 4[Issue]) OR (&quot;Frontiers in physiology&quot;[Journal] AND (&quot;2018/04/01&quot;[Date—Publication]: &quot;2018/04/30&quot;[Date—Publication])) OR (&quot;The international journal of behavioral nutrition and physical activity&quot;[Journal] AND (&quot;2018/04/01&quot;[Date—Publication]: &quot;2018/04/30&quot;[Date—Publication])).\n\nPlant science: (&quot;Nature plants&quot;[Journal] AND 4[Issue] AND 4[Volume]) OR (&quot;Molecular plant&quot;[Journal] AND 4[Issue] AND 11[Volume]) OR (&quot;The Plant cell&quot;[Journal] AND 4[Issue] AND 30[Volume]) OR (&quot;Plant biotechnology journal&quot;[Journal] AND 4[Issue] AND 16[Volume]) OR (&quot;The New phytologist&quot;[Journal] AND (1[Issue] OR 2[Issue]) AND 218[Volume]) OR (&quot;Plant physiology&quot;[Journal] AND 4[Issue] AND 176[Volume]) OR (&quot;Plant, cell &amp; environment&quot;[Journal] AND 4[Issue] AND 41[Volume]) OR (&quot;The Plant journal: for cell and molecular biology&quot;[Journal] AND (1[Issue] OR 2[Issue]) AND 94[Volume]) OR (&quot;Journal of experimental botany&quot;[Journal] AND (8[Issue] OR 9[Issue] OR 10[Issue]) AND 69[Volume]) OR (&quot;Plant &amp; cell physiology&quot;[Journal] AND 4[Issue] AND 59[Volume]) OR (&quot;Molecular plant pathology&quot;[Journal] AND 4[Issue] AND 19[Volume]) OR (&quot;Environmental and experimental botany&quot;[Journal] AND 148[Volume]) OR (&quot;Molecular plant-microbe interactions: MPMI&quot;[Journal] AND 4[Issue] AND 31[Volume]) OR (“Frontiers in plant science”[Journal] AND (&quot;2018/04/01&quot;[Date—Publication]: &quot;2018/04/30&quot;[Date—Publication])) OR (“The Journal of ecology” (&quot;2018/04/01&quot;[Date—Publication]: &quot;2018/04/30&quot;[Date—Publication])).\n\nCell biology: (&quot;Cell&quot;[Journal] AND (2[Issue] OR 3[Issue]) AND 173[Volume]) OR (&quot;Nature medicine&quot;[Journal] AND 24[Volume] AND 4[Issue]) OR (&quot;Cancer cell&quot;[Journal] AND 33[Volume] AND 4[Issue]) OR (&quot;Cell stem cell&quot;[Journal] AND 22[Volume] AND 4[Issue]) OR (&quot;Nature cell biology&quot;[Journal] AND 20[Volume] AND 4[Issue]) OR (&quot;Cell metabolism&quot;[Journal] AND 27[Volume] AND 4[Issue]) OR (&quot;Science translational medicine&quot;[Journal] AND 10[Volume] AND (435[Issue] OR 436[Issue] OR 437[Issue] OR 438[Issue])) OR (&quot;Cell research&quot;[Journal] AND 28[Volume] AND 4[Issue]) OR (&quot;Molecular cell&quot;[Journal] AND 70[Volume] AND (1[Issue] OR 2[Issue])) OR(&quot;Nature structural &amp; molecular biology&quot;[Journal] AND 25[Volume] AND 4[Issue]) OR (&quot;The EMBO journal&quot;[Journal] AND 37[Volume] AND (7[Issue] OR 8[Issue])) OR (&quot;Genes &amp; development&quot;[Journal] AND 32[Volume] AND 7–8[Issue]) OR (&quot;Developmental cell&quot;[Journal] AND 45[Volume] AND (1[Issue] OR 2[Issue])) OR (&quot;Current biology: CB&quot;[Journal] AND 28[Volume] AND (7[Issue] OR 8[Issue])) OR (&quot;Plant cell&quot;[Journal] AND 30[Volume] AND 4[Issue]).\n\nScreening\n\nScreening for each article was performed by 2 independent reviewers (Physiology: TLW, SS, EMW, VI, KW, MO; Plant science: TLW, SJB; Cell biology: EW, SS) using Rayyan software (RRID:SCR_017584), and disagreements were resolved by consensus. A list of articles was uploaded into Rayyan. Reviewers independently examined each article and marked whether the article was included or excluded, along with the reason for exclusion. Both reviewers screened all articles published in each journal between April 1 and April 30, 2018, to identify full length, original research articles (S1–S3 Tables, S1 Fig) published in the print issue of the journal. Articles for online journals that do not publish print issues were included if the publication date was between April 1 and April 30, 2018. Articles were excluded if they were not original research articles, or if an accepted version of the paper was posted as an “in press” or “early release” publication; however, the final version did not appear in the print version of the April issue. Articles were included if they contained at least one eligible image, such as a photograph, an image created using a microscope or electron microscope, or an image created using a clinical imaging technology such as ultrasound or MRI. Blot images were excluded, as many of the criteria in our abstraction protocol cannot easily be applied to blots. Computer generated images, graphs, and data figures were also excluded. Papers that did not contain any eligible images were excluded.\n\nAbstraction\n\nAll abstractors completed a training set of 25 articles before abstracting data. Data abstraction for each article was performed by 2 independent reviewers (Physiology: AA, AV; Plant science: MO, TLA, SA, KW, MAG, IF; Cell biology: IF, AA, AV, KW, MAG). When disagreements could not be resolved by consensus between the 2 reviewers, ratings were assigned after a group review of the paper. Eligible manuscripts were reviewed in detail to evaluate the following questions according to a predefined protocol (available at: https://doi.org/10.17605/OSF.IO/B5296) [14]. Supplemental files were not examined, as supplemental images may not be held to the same peer review standards as those in the manuscript.\n\nThe following items were abstracted:\n\nTypes of images included in the paper (photograph, microscope image, electron microscope image, image created using a clinical imaging technique such as ultrasound or MRI, other types of images)\n\nDid the paper contain appropriately labeled scale bars for all images?\n\nWere all insets clearly and accurately marked?\n\nWere all insets clearly explained in the legend?\n\nIs the species and tissue, object, or cell line name clearly specified in the figure or legend for all images in the paper?\n\nAre any annotations, arrows, or labels clearly explained for all images in the paper?\n\nAmong images where authors can control the colors shown (e.g., fluorescence microscopy), are key features of the images visible to someone with the most common form of colorblindness (deuteranopia)?\n\nIf the paper contains colored labels, are these labels visible to someone with the most common form of color blindness (deuteranopia)?\n\nAre colors in images explained either on the image or within the legend?\n\nQuestions 7 and 8 were assessed by using Color Oracle [36] (RRID:SCR_018400) to simulate the effects of deuteranopia.\n\nVerification\n\nTen percent of articles in each field were randomly selected for verification abstraction, to ensure that abstractors in different fields were following similar procedures. Data were abstracted by a single abstractor (TLW). The question on species and tissue was excluded from verification abstraction for articles in cell biology and plant sciences, as the verification abstractor lacked the field-specific expertise needed to assess this question. Results from the verification abstractor were compared with consensus results from the 2 independent abstractors for each paper, and discrepancies were resolved through discussion. Error rates were calculated as the percentage of responses for which the abstractors’ response was incorrect. Error rates were 5% for plant sciences, 4% for physiology, and 3% for cell biology.\n\nData processing and creation of figures\n\nData are presented as n (%). Summary statistics were calculated using Python (RRID:SCR_008394, version 3.6.9, libraries NumPy 1.18.5 and Matplotlib 3.2.2). Charts were prepared with a Python-based Jupyter Notebook (Jupyter-client, RRID:SCR_018413 [37], Python version 3.6.9, RRID:SCR_008394, libraries NumPy 1.18.5 [38], and Matplotlib 3.2.2 [39]) and assembled into figures with vector graphic software. Example images were previously published or generously donated by the manuscript authors as indicated in the figure legends. Image acquisition was described in references (D. melanogaster images [18], mouse pancreatic beta islet cells: A. Müller personal communication, and Orobates pabsti [19]). Images were cropped, labeled, and color-adjusted with FIJI [15] (RRID:SCR_002285) and assembled with vector-graphic software. Colorblind and grayscale rendering of images was done using Color Oracle [36] (RRID:SCR_018400). All poor and clear images presented here are “mock examples” prepared based on practices observed during data abstraction.\n\nSupporting information"
}