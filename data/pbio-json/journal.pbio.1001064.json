{
  "schema": "https://data.sciveyor.com/schema",
  "version": 5,
  "id": "doi:10.1371/journal.pbio.1001064",
  "doi": "10.1371/journal.pbio.1001064",
  "externalIds": [
    "pii:PBIOLOGY-D-11-00010",
    "pmid:21610856",
    "pmcid:PMC3096604"
  ],
  "license": "This is an open-access article distributed under the\n                terms of the Creative Commons Attribution License, which permits unrestricted use,\n                distribution, and reproduction in any medium, provided the original author and\n                source are credited.",
  "licenseUrl": "https://creativecommons.org/licenses/by/4.0/",
  "dataSource": "Public Library of Science",
  "dataSourceUrl": "https://data.sciveyor.com/source/plos",
  "dataSourceVersion": 1,
  "type": "article",
  "title": "Cracking the Code of Oscillatory Activity",
  "authors": [
    {
      "name": "Philippe G. Schyns",
      "first": "Philippe G.",
      "last": "Schyns",
      "affiliation": "Institute of Neuroscience and Psychology, University of Glasgow, Glasgow,\n                    United Kingdom"
    },
    {
      "name": "Gregor Thut",
      "first": "Gregor",
      "last": "Thut",
      "affiliation": "Institute of Neuroscience and Psychology, University of Glasgow, Glasgow,\n                    United Kingdom"
    },
    {
      "name": "Joachim Gross",
      "first": "Joachim",
      "last": "Gross",
      "affiliation": "Institute of Neuroscience and Psychology, University of Glasgow, Glasgow,\n                    United Kingdom"
    }
  ],
  "journal": "PLoS Biology",
  "date": "2011-05",
  "dateAccepted": "2011-04-07",
  "dateReceived": "2010-12-30",
  "volume": "9",
  "number": "5",
  "pages": "e1001064",
  "tags": [
    "Discipline-v2/Information science",
    "Discipline-v2/Information theory",
    "Discipline-v2/Medicine",
    "Discipline-v2/Neuroimaging",
    "Discipline-v2/Neurology",
    "Discipline-v2/Social and behavioral sciences",
    "Discipline/Neurological Disorders",
    "Information science",
    "Information theory",
    "Neuroimaging",
    "Neurology",
    "Type/Research Article"
  ],
  "abstract": "Neural oscillations are ubiquitous measurements of cognitive processes and\n                    dynamic routing and gating of information. The fundamental and so far unresolved\n                    problem for neuroscience remains to understand how oscillatory activity in the\n                    brain codes information for human cognition. In a biologically relevant\n                    cognitive task, we instructed six human observers to categorize facial\n                    expressions of emotion while we measured the observers' EEG. We combined\n                    state-of-the-art stimulus control with statistical information theory analysis\n                    to quantify how the three parameters of oscillations (i.e., power, phase, and\n                    frequency) code the visual information relevant for behavior in a cognitive\n                    task. We make three points: First, we demonstrate that phase codes considerably\n                    more information (2.4 times) relating to the cognitive task than power. Second,\n                    we show that the conjunction of power and phase coding reflects detailed visual\n                    features relevant for behavioral response—that is, features of facial\n                    expressions predicted by behavior. Third, we demonstrate, in analogy to\n                    communication technology, that oscillatory frequencies in the brain multiplex\n                    the coding of visual features, increasing coding capacity. Together, our\n                    findings about the fundamental coding properties of neural oscillations will\n                    redirect the research agenda in neuroscience by establishing the differential\n                    role of frequency, phase, and amplitude in coding behaviorally relevant\n                    information in the brain.",
  "fullText": "Introduction\n      \nInvasive and noninvasive studies in humans under physiological and pathological\n                conditions converged on the suggestion that the amplitude and phase of neural\n                oscillations implement cognitive processes such as sensory representations,\n                attentional selection, and dynamical routing/gating of information [1]–[4]. Surprisingly,\n                most studies have ignored how the temporal dynamics of phase code the sensory\n                stimulus, focusing instead on amplitude envelopes (but see [5]), relations between amplitude and\n                frequency [6],\n                or coupling between frequencies ([7]–[10]; see [11] for a review). But there is compelling evidence that\n                phase dynamics of neural oscillations are functionally relevant [12]–[16].\n                Furthermore, computational arguments suggest that if brain circuits performed\n                efficient amplitude-to-phase conversion [17],[18], temporal phase coding could\n                be advantageous in fundamental operations such as object representation and\n                categorization by implementing efficient winner-takes-all algorithms [17], by providing\n                robust sensory representations in unreliable environments, and by lending themselves\n                to multiplexing, an efficient mechanism to increase coding capacity [18],[19]. To crack the\n                code of oscillatory activity in human cognition, we must tease apart the relative\n                contribution of frequency, amplitude, and phase to the coding of behaviorally\n                relevant information.\n\nWe instructed six observers to categorize faces according to six basic expressions of\n                emotion (“happy,” “fear,” “surprise,”\n                “disgust,” “anger,” “sad,” plus\n                “neutral”). We controlled visual information, by presenting on each\n                trial a random sample of face information—smoothly sampled from the image\n                using Gaussian apertures at different spatial frequency bands. The Gaussian\n                apertures randomly sampled face parts simultaneously across the two dimensions of\n                the image and the third dimension of spatial frequency bands (Figure S1\n                illustrates the sampling process for one illustrative trial; [20],[21]). We recorded the\n                observers' categorization and EEG responses to these samples (see Materials and Methods, Procedure).\n\nTo quantify the relative coding properties of power, phase, and frequency, we used\n                state-of-the-art information theoretic methods (Mutual Information,\n                    MI, which measures the mutual dependence between two variables;\n                    [22]) and\n                computed three different MI measurements: between sampled pixel\n                information and behavioral responses to each emotion category (correct versus\n                incorrect), between EEG responses (for power, phase, and the conjunction of phase\n                and power) and behavior, and finally between sampled pixel information and EEG\n                response (see Figure\n                    S2 for the mutual information analysis framework and Computation: Mutual\n                Information).\n\n      Results\n      \nFirst, to characterize the information that the brain processes in the cognitive\n                task, for each observer and category, we computed MI(Pixel;\n                Behavior), the MI between the distribution of grey-level values of\n                each image pixel (arising from the summed Gaussian masks across spatial frequency\n                bands, down-sampled from a 380×240 pixels image to a 38 to 24 image and\n                gathered across trials) and equal numbers of correct versus incorrect categorization\n                responses. Figure 1,\n                    MI(Pixel; Behavior) illustrates MI on a scale\n                from 0 to 0.05 bits. High values indicate the face pixels (e.g., forming the mouth\n                in “happy”) representing the visual information that the brain must\n                process to correctly categorize the stimuli (see Figure S3 for a\n                detailed example of the computation).\n\nWe now compare how the parameters of oscillatory frequency, power, and phase code\n                this information in the brain. For each observer, expression, electrode of the\n                standard 10–20 position system, and trial, we performed a Time ×\n                Frequency decomposition of the signal sampled at 1,024 Hz, with a Morlet wavelet of\n                size 5, between −500 and 500 ms around stimulus onset and every 2 Hz between 4\n                and 96 Hz. We make three points:\n\n(a) The conjunction of phase and power (phase&amp;power) codes more\n                    information about complex categorization tasks than phase and power on their\n                    own. In Figure 2,\n                    MI(EEG response; Behavior) measures the reduction of\n                uncertainty of the brain response, when the behavioral variable correct versus\n                incorrect categorization is known. We provide the measure for each electrode of the\n                standard 10–20 position system over the Time × Frequency space. Pz, Oz,\n                P8, and P7 had highest MI values of all electrodes, irrespective of\n                whether the brain response considered was power (blue box), phase (green box), or\n                the phase&amp;power (red box). The adjacent MI scales reveal that\n                phase&amp;power was 1.25 times more informative of behavior than phase, itself 2.4\n                times more informative than power. Phase&amp;power was 3 times more informative than\n                power alone. Henceforth, the analyses focus on these four electrodes and on\n                phase&amp;power, the most informative brain measurement for the cognitive task.\n\n(b) Phase&amp;power codes detailed categorization-relevant features of\n                    sensory stimuli. MI(Pixel; Behavior) revealed that the two eyes and the\n                mouth are prominent features of expression discrimination (see Figure 1). As explained, with Gaussian masks we\n                sampled pixels from the face on each trial. Consequently, for all correct trials of\n                an expression category (e.g., “happy”), we can measure at each pixel\n                location the mutual information between the distribution of grey-level values of the\n                Gaussian masks across trials and each cell of the Time × Frequency brain\n                response. Figure 3 reports\n                    MI(Pixel; Phase&amp;Power), focusing on Pz, Oz, P8, and P7. The\n                red box represents, at 4 Hz and 156 ms, following stimulus onset (a time point\n                chosen for its prominence in face coding [21]), the color-coded\n                    MI value of each face pixel—overlayed on a neutral face\n                background for ease of feature interpretation (the yellow box presents mutual\n                information at 12 Hz and 156 ms). The scale is the adjacent rainbow colors ranging\n                from 0 to 0.03 bits. Electrodes P7 (over left occipito-temporal cortex) and P8 (over\n                right occipital-temporal cortex) reveal the highest MI to the\n                contra-lateral eye (i.e., left eye for P8; right eye for P7). At the same time on Pz\n                and Oz, the highest MI is to both eyes and to the mouth.\n\nTo generalize across Time × Frequency, for ease of presentation, we computed\n                three masks extracting pixel locations from the left eye, right eye, and mouth. We\n                averaged MI values within each mask, independently for each Time\n                × Frequency cell. We then color-coded MI for each feature in\n                RGB color space—red for “right eye,” green for\n                “mouth,” and blue for “left eye”; see schematic colored\n                faces adjacent to the Time × Frequency plot for complete color coding. The\n                broad red (versus blue) cloud on electrode P7 (versus P8) denotes highest\n                    MI to the right (versus left) eye in this Time ×\n                Frequency region, whereas Pz and Oz demonstrate sensitivity to the two eyes (in\n                purple) and to the mouth (in green). To conclude, phase&amp;power codes detailed\n                categorization-relevant features of the sensory input.\n\n(c) Phase&amp;power coding is multiplexed across oscillatory\n                    frequencies. Theta (4 Hz) and low beta (12 Hz) on both Oz and Pz\n                demonstrate the remarkable multiplexing property of phase&amp;power coding: the idea\n                that the brain codes different information in different oscillatory bands. In Figure 3, Oz and Pz reveal that\n                beta encodes two eyes (see the purple RGB code and the yellow framed faces) when\n                theta encodes the mouth (see the green RGB code and the red framed faces).\n                Multiplexing is also present to a lesser degree on P8 and P7. MI\n                values critically depend on the joint distribution of variables (see Figure S3), and\n                so we turn to Figure 4 to\n                understand how the variables of phase and power jointly contribute to the coding of\n                facial features. Figure 4\n                develops the red and yellow framed faces of Figure 3, for electrode Pz. At 156 ms, at 4 and\n                12 Hz, we discretized the distribution of power and phase neural responses in\n                3×3 bins—represented in Cartesian coordinates as\n                        . In each bin, we averaged the pixel values leading to this\n                range of imaginary numbers. At 12 Hz, what emerges is a phase&amp;power coding of\n                the two eyes (in red, between 45 and 90 deg of phase) and an encoding of the mouth\n                (in red, between 270 and 315 deg of phase). At 4 Hz, the encoding of mostly the\n                mouth and the two eyes (in red) occurs between 90 and 135 deg of phase. The 4 and 12\n                Hz colored boxes in Figure 4\n                therefore illustrate the prominence of phase coding for facial features.\n\n      Discussion\n      \nHere, using the concept of mutual information from Information Theory, we compared\n                how the three parameters of neural oscillations (power, phase, and frequency)\n                contribute to the coding of information in the biologically relevant cognitive task\n                of categorizing facial expressions of emotion. We demonstrated that phase codes 2.4\n                times more information about the task than power. The conjunction of power and phase\n                (itself 3 times more informative than power) codes specific expressive features\n                across different oscillatory bands, a multiplexing that increases coding capacity in\n                the brain.\n\nIn general, the relationship between our results on the frequency, power, and phase\n                coding of neural oscillations cannot straightforwardly be related to the coding\n                properties of more standard measures of the EEG such as event related potentials\n                (ERP). However, an identical experimental protocol was run on the N170\n                face-sensitive potential [21],[23], but using reverse correlation analyses, not MI. Sensor\n                analyses revealed that the N170 ERP initially coded the eye contra-lateral to the\n                sensor considered, for all expressions, followed at the N170 peak by a coding of the\n                behaviorally relevant information [21], together with a more detailed coding of features (i.e.,\n                with their Higher Spatial Frequencies) at the peak [23]. Interestingly, distance\n                of behaviorally relevant information (e.g., the wide-opened eyes in\n                “fearful” versus the mouth in “happy”) to the initially\n                coded eye determined the latency of the N170 peak (with the ERP to a\n                “happy” face peaking later than to a “fearful” face). ERPs\n                confer the advantage of precise timing, leading to precise time course of coding in\n                the brain, including phase differences across visual categories. However, we do not\n                know whether this coding occurs over one or multiple sources of a network that might\n                oscillate at different temporal frequencies (as suggested here between theta and\n                beta), for example to code features at different spatial resolutions (as suggested\n                in [19] and [24]). In sum, the\n                complex relations between EEG/MEG data, the underlying cortical networks of sources,\n                their oscillatory behaviors, and the coding of behaviorally relevant features at\n                different spatial resolutions open a new range of fundamental questions. Resolving\n                these questions will require integration of existing methods, as none of them is\n                singly sufficient.\n\nIn these endeavors, the phase and frequency multiplexing coding properties of neural\n                oscillations cannot be ignored.\n\n      Materials and Methods\n      \n        Participants\n        \nSix observers from Glasgow University, UK, were paid to take part in the\n                    experiment. All had normal vision and gave informed consent prior to\n                    involvement. Glasgow University Faculty of Information and Mathematical Sciences\n                    Ethics Committee provided ethical approval.\n\n        Stimuli\n        \nOriginal face stimuli were gray-scale images of five females and five males taken\n                    under standardized illumination, each displaying seven facial expressions. All\n                    70 stimuli (normalized for the location of the nose and mouth) complied with the\n                    Facial Action Coding System (FACS, [25]) and form part of the\n                    California Facial Expressions (CAFE) database [26]. As facial information is\n                    represented at multiple spatial scales, on each trial we exposed the visual\n                    system to a random subset of Spatial Frequency (SF) information contained within\n                    the original face image. To this end, we first decomposed the original image\n                    into five non-overlapping SF bands of one octave each (120–60,\n                    60–30, 30–15, 15–7.5, and 7.5–3.8 cycles/face, see Figure S1).\n                    To each SF band, we then applied a mask punctured with Gaussian apertures to\n                    sample SF face information with “bubbles.” These were positioned in\n                    random locations trial by trial, approximating a uniform sampling of all face\n                    regions across trials. The size of the apertures was adjusted for each SF band,\n                    so as to reveal six cycles per face. In addition, the probability of a bubble in\n                    each SF band was adjusted so as to maintain constant the total area of face\n                    revealed (standard deviations of the bubbles were 0.36, 0.7, 1.4, 2.9, and 5.1\n                    cycles/degree of visual angle from the fine to the coarse SF band). Calibration\n                    of the sampling density (i.e., the number of bubbles) was performed online on a\n                    trial-by-trial basis to maintain observer's performance at 75%\n                    correct categorization independently for each expression. The stimulus presented\n                    on each trial comprised the randomly sampled information from each SF band\n                    summed together [27].\n\n        Procedure\n        \nPrior to testing, observers learned to categorize the 70 original images into the\n                    seven expression categories. Upon achieving a 95% correct classification\n                    criterion of the original images, observers performed a total of 15 sessions of\n                    1,400 trials (for a total of 21,000 trials) of the facial expressions\n                    categorization task (i.e., 3,000 trials per expression, happy, sad, fearful,\n                    angry, surprised, disgusted, and neutral faces, randomly distributed across\n                    sessions). Short breaks were permitted every 100 trials of the experiment.\n\nIn each trial a 500 ms fixation cross (spanning 0.4° of visual angle) was\n                    immediately followed by the sampled face information, as described before (see\n                        Figure\n                        S1). Stimuli were presented on a light gray background in the centre\n                    of a monitor; a chin-rest maintained a fixed viewing distance of 1 m (visual\n                    angle 5.36°×3.7° forehead to base of chin). Stimuli remained on\n                    screen until response. Observers were asked to respond as quickly and accurately\n                    as possible by pressing expression-specific response keys (seven in total) on a\n                    computer keyboard.\n\n        EEG Recording\n        \nWe recorded scalp electrical activity of the observers while they performed the\n                    task. We used sintered Ag/AgCl electrodes mounted in a 62-electrode cap\n                    (Easy-Cap) at scalp positions including the standard 10–20 system\n                    positions along with intermediate positions and an additional row of low\n                    occipital electrodes. Linked mastoids served as initial common reference and\n                    electrode AFz as the ground. Vertical electro-oculogram (vEOG) was bipolarly\n                    registered above and below the dominant eye and the horizontal electro-oculogram\n                    (hEOG) at the outer canthi of both eyes. Electrode impedance was maintained\n                    below 10 kΩ throughout recording. Electrical activity was continuously\n                    sampled at 1,024 Hz. Analysis epochs were generated off-line, beginning 500 ms\n                    prior to stimulus onset and lasting for 1,500 ms in total. We rejected EEG and\n                    EOG artefacts using a [−30 µV; +30 µV]\n                    deviation threshold over 200 ms intervals on all electrodes. The EOG rejection\n                    procedure rejected rotations of the eyeball from 0.9 deg inward to 1.5 deg\n                    downward of visual angle—the stimulus spanned 5.36°×3.7° of\n                    visual angle on the screen. Artifact-free trials were sorted using EEProbe (ANT)\n                    software, narrow-band notch filtered at 49–51 Hz, and re-referenced to\n                    average reference.\n\n        Computation: Mutual Information\n        \nIn Information Theory [28],[29], Mutual Information\n                        MI(X;Y ) between random\n                    variables X and Y measures their mutual\n                    dependence. When logarithms to the base 2 are used in Equation 1, the unit of\n                    mutual information is expressed in bits.\n\nThe critical term is p(x,y),\n                    the joint probabilities between X and Y. When the variables are\n                    independent, the logarithm term in Equation 1 becomes 0 and\n                        MI(X;Y\n                    ) = 0. In contrast, when X and\n                        Y are dependent\n                        MI(X;Y ) returns a value\n                    in bits that quantifies the mutual dependence between X and\n                        Y. Derived from the measure of uncertainty of a random\n                    variable X expressed in Equation 2 and the conditional\n                    uncertainty of two random variables X and Y\n                    (Equation 3),\n\nMutual Information measures how much bits of information X and\n                        Y share. It quantifies the reduction of uncertainty about\n                    one variable that our knowledge of the other variable induces (Equation\n                            4),\n\nHere, we use Mutual Information to measure the mutual dependence between the\n                    sampling of input visual information from faces and the oscillatory brain\n                    responses to these samples and between the same input information and behavior\n                    (see Figure\n                        S2 for an overall illustration of our framework; see Figure S3\n                    for a detailed development of the computations between face pixels and correct\n                    versus incorrect behavioral responses). For all measures of MI, we used the\n                    direct method with quadratic extrapolation for bias correction [22]. We\n                    quantized data into four equi-populated bins, a distribution that maximizes\n                    response entropy [22]. Results were qualitatively similar for a larger\n                    number of bins (tested in the range of 4 to 16). Below, we provide details for\n                    the computation of mutual information with behavioural and EEG responses,\n                    including number of trials taken into consideration for the MI computations and\n                    the determination of statistical thresholds of mutual information.\n\n        Behavioral Mutual Information, MI(Pixel; Behavior)\n        \nOn each of the 21,000 trials of a categorization task, the randomly located\n                    Gaussian apertures make up a three-dimensional mask that reveals a sparse face.\n                    Observers will tend to be correct when this sampled SF information is diagnostic\n                    for the categorization of the considered expression. To identify the face\n                    features used for each facial expression categorization, we computed mutual\n                    information, per observer, between the grey levels of each face pixels and a\n                    random sample of correct matching the number of incorrect trials (i.e., on\n                    average 5,250 correct trials and 5,250 incorrect trials). For each expression,\n                    we then averaged mutual information values across all six observers,\n                    independently for each pixel. To establish statistical thresholds, we repeated\n                    the computations 500 times for each pixel, after randomly shuffling the order of\n                    response—to disrupt the association between pixel values and\n                    categorization responses. For each of the 500 computations, we selected the\n                    maximum mutual information value across all pixels. We then chose as statistical\n                    threshold the 99th percentile of the distribution of maxima. This maximum\n                    statistic implements a correction for multiple comparisons because the\n                    permutation provides the null distribution of the maximum statistical value\n                    across all considered dimensions [30]. Behavioral mutual\n                    information is reported as the top row of faces in Figure 1.\n\n        EEG Mutual Information\n        \nHere, we examined two different measures: MI(EEG Response;\n                    Behavior) and MI(Pixel; EEG Response). MI(EEG\n                    Response; Behavior) computed, for each electrode, subject, and expression, the\n                    mutual information between correct and incorrect trials and the power, phase,\n                    and phase&amp;power of the Time × Frequency EEG signal. For this\n                    computation, we used the same number of trials as for Behavior MI (i.e., on\n                    average 5,250 correct trials and 5,250 incorrect trials). As with behavior, for\n                    each electrode and type of EEG measurement, we averaged the mutual information\n                    values across subjects and expression. To establish statistical thresholds, we\n                    repeated the computations 500 times, permuting the trial order of the EEG Time\n                    × Frequency values and identified the 500 maxima each time across the\n                    entire Time × Frequency space. We identified the statistical threshold as\n                    the 99th percentile of the distribution of maxima (see Figure 2).\n\nMI(Pixel; Phase&amp;Power) computed, for each subject,\n                    expression, and face pixel (down-sampled to 38×24 pixel maps), the mutual\n                    information between the distribution of each face pixel grey-level value and the\n                    most informative of the brain responses, phase&amp;power Time × Frequency\n                    responses, for correct trials only. That is, an average of 15,750 trials per\n                    subject. To establish statistical thresholds, given the magnitude of the\n                    computation, we computed z scores using the pre-stimulus\n                    presentation baseline (from −500 to 0 ms) to estimate mean and standard\n                    deviation. In Figure 3, .01\n                    bits of mutual information correspond to a z score of 55.97, so\n                    all mutual information values this number of bits (see the level marked with an\n                    asterisk in Figure 3) are\n                    well above an uncorrected threshold of .0000001 (itself associated with a\n                        z score of 5).\n\nFigure 2 indicated two\n                    clusters of maximal MI in all three measures (Power, Phase, and\n                    Phase&amp;Power) at a latency of 140–250 ms in two frequency bands (4 Hz\n                    and 12–14 Hz). We averaged the MI measures, for each\n                    cluster, electrode, and subject, and subjected these MI\n                    averages to a two-way ANOVA with factors electrode (P7, P8, Pz, and Oz) and\n                    measure (Power, Phase, and Phase&amp;Power). Both clusters revealed a\n                    significant main effect of electrode (F(1,\n                    3) = 8.38, p&lt;0.001 for 4 Hz and\n                        F(1, 3) = 79.34,\n                    p&lt;0.001 for 12–14 Hz) and measure\n                    (F(1, 2) = 44.24,\n                    p&lt;0.001 for 4 Hz and F(1,\n                    2) = 104.77, p&lt;0.001 for 12–14\n                    Hz). Post hoc t test confirmed that\n                    MI(Phase&amp;Power) is significantly higher than\n                        MI(Phase)\n                    (p = 0.013), which itself is significantly\n                    higher than MI(Power)\n                    (p = 0.003).\n\n      Supporting Information"
}